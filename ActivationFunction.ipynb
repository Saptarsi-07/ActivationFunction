{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c1cb1696-f6f0-4a55-9ed5-3473614acf80",
   "metadata": {},
   "source": [
    "Q1) Activation function introduces non-linearity, allowing the network to learn complex patterns and solve intricate tasks."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6e04e80-e209-44fb-934a-af0fcb679d64",
   "metadata": {},
   "source": [
    "Q2)  Common activation functions include Sigmoid, ReLU, Tanh, and Softmax."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a640a74c-a3e2-4122-8212-fea9e0bf5d68",
   "metadata": {},
   "source": [
    "Q3) Activation functions affect the training process and performance of a neural network by introducing non-linearity, influencing gradient flow, preventing dead neurons, impacting convergence speed, determining output range, and overcoming saturation issues. Choosing appropriate activation functions is crucial for optimal performance."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a87e524-ad1c-4717-9592-a34b38be0c54",
   "metadata": {},
   "source": [
    "Q4) The sigmoid activation function maps the input to a value between 0 and 1. Its formula is f(x) = 1 / (1 + exp(-x)), where 'x' is the input to the function. \n",
    "Advantages: It squashes the input into a bounded range (0 to 1).\n",
    "            It is smooth and differentiable everywhere, which allows for easy computation of gradients during backpropagation\n",
    "Disadvantages: It suffers from the vanishing gradient problem: For very large positive or negative inputs, the gradient approaches zero.\n",
    "               Sigmoid outputs are not symmetric around zero, leading to the \"bias shift\" problem.\n",
    "               It can only be applied for binary problems"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbd87a55-39c0-4251-9e44-4aa1b1ed0452",
   "metadata": {},
   "source": [
    "Q5) The Rectified Linear Unit (ReLU) activation function is a non-linear function used in neural networks. It sets negative inputs to zero and leaves positive inputs unchanged. It differs from the sigmoid function in that ReLU is non-saturating, leading to faster training and mitigating the vanishing gradient problem. ReLU is widely used in deep learning due to its simplicity and better performance compared to sigmoid."
   ]
  },
  {
   "cell_type": "raw",
   "id": "920070a6-5abd-480f-b71c-3e59cc2978ae",
   "metadata": {},
   "source": [
    "Q6) i)  We can avoid vanishing gradient issue of sigmoid\n",
    "    ii) Faster training\n",
    "    iii)Avoid dead neurons and bias shifting issue"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fb8617e-20ca-4d80-a158-7aa67ae5dd97",
   "metadata": {},
   "source": [
    "Q7) Leaky ReLU is a variant of the Rectified Linear Unit (ReLU) activation function that addresses the vanishing gradient problem, which can occur with standard ReLU. The Leaky ReLU function is defined as f(x) = max(ax, x), where 'a' is a small positive constant and 'x' is the input."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c35d53bb-dc4a-4d89-a433-fd82b606a424",
   "metadata": {},
   "source": [
    "Q8) The purpose of the softmax activation function is to convert a vector of real numbers into a probability distribution. It takes an input vector and transforms each element into a positive value between 0 and 1, with the sum of all elements equaling 1. This normalization property allows the softmax function to represent the relative likelihoods of different classes in a multi-class classification problem."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f24331f2-9a0f-4b31-bfd6-d10fdefb415b",
   "metadata": {},
   "source": [
    "Q9) The hyperbolic tangent (tanh) activation function is a non-linear function that maps the input to a value between -1 and 1. Its formula is f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)), where 'x' is the input to the function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
